# 背景

记录工作中、学习中未有系统时间进行学习的知识点，或者碰到的知识，持续学习可以从这里开始。



# 内容

------

QWen2.5的地址：

github：https://github.com/QwenLM/Qwen2.5?tab=readme-ov-file#-modelscope

官方文档：https://qwenlm.github.io/

------

Pytorch官网：https://pytorch.org/get-started/locally/

------

ollama地址：

- Ollama 下载：https://ollama.com/download
- Ollama 官方主页：[https://ollama.com](https://ollama.com/)
- Ollama 官方 GitHub 源代码仓库：[https://github.com/ollama/ollama/](https://github.com/ollama/ollama)
- Ollama仓库：https://ollama.com/library

最简安装：

```bash
# 下载并直接安装
curl -fsSL https://ollama.com/install.sh | sh
# 运行ollama服务
ollama serve
```

模型存放位置：

- macOS: `~/.ollama/models`
- Linux: `/usr/share/ollama/.ollama/models`
- Windows: `C:\Users\<username>\.ollama\models`

Ollama 从最新版0.3.13开始支持从 Huggingface Hub 上直接拉取各种模型，包括社区创建的 GGUF 量化模型。用户可以通过简单的命令行指令快速运行这些模型。

ollama run hf.co/{username}/{repository}

![image-20241023150948772](C:\Users\intple\AppData\Roaming\Typora\typora-user-images\image-20241023150948772.png)

![image-20241023151017889](C:\Users\intple\AppData\Roaming\Typora\typora-user-images\image-20241023151017889.png)



可以直接从hf里面的如下位置找到ollama run的命令：

![image-20241023151404236](C:\Users\intple\AppData\Roaming\Typora\typora-user-images\image-20241023151404236.png)



然后选择对应的模型，copy：

![image-20241023151502469](C:\Users\intple\AppData\Roaming\Typora\typora-user-images\image-20241023151502469.png)



一个学习Ollama的好地方：

https://github.com/datawhalechina/handy-ollama?tab=readme-ov-file



Ollama默认提供的http://ip:11434/api/xxx接口只能本机访问。外部不行，需要在/etc/systemd/system/ollama.service文件service下面增加一行：

![image-20241024100955123](C:\Users\intple\AppData\Roaming\Typora\typora-user-images\image-20241024100955123.png)

这样其他机器才能用这个url调用处于run状态的大模型。





------

自动化问答生成：使用GPT-3.5将文档转化为问答对：

https://blog.csdn.net/u012960155/article/details/132658756

测试下来，ollama run qwen2.5:7b，这个版本还可以，但是7b其他的量化版本好像就不太行。

如下是给出一段文字，通过各种prompt提示，模型给出的问答对。

![image-20241023152230078](C:\Users\intple\AppData\Roaming\Typora\typora-user-images\image-20241023152230078.png)

------

先有应用，应用关联大模型、知识库。

用户：管理员，普通用户

用户管理和maxkb差不多。

模型管理，支持canone的模型，配置大模型就配置canone已经发布的大猫熊，类型就是支持的，里面

maxkb知识库要调研一下他的实现方式，看怎么哦同步我们的训练数据集。文档要

用户监控，权限。

4.181上密码：

admin/Maxkb@123



------

安装python3.10+ 后安装其他包如果报：

![0b080a12537581b050251f6ba852b8a](C:\Users\intple\Documents\WeChat Files\wxid_so12ob98wb8y21\FileStorage\Temp\0b080a12537581b050251f6ba852b8a.png)

也就是你想从网络上安装任何包都不行。那是因为你安装python缺少openssl1.1.1以上的包。必须先安装好openssl。

推荐：

https://cloud.tencent.com/developer/article/2424089



------

https://maxkb.cn/docs/dev_manual/dev_environment/

MaxKB的文档

------

poetry增加源方法：

poetry source add --priority=default mirrors https://pypi.tuna.tsinghua.edu.cn/simple/

然后再执行poetry install。。。

------

https://ymzhang-cs.github.io/posts/wsl-configuration/，从零开始配置WSL2下的Python开发环境，看这一篇就够了

https://www.cnblogs.com/legendjslc/p/12667967.html，[PyCharm使用WSL开发](https://www.cnblogs.com/legendjslc/p/12667967.html)

https://www.jetbrains.com/help/pycharm/using-wsl-as-a-remote-interpreter.html：pycharm支持wsl（必须是专业版）

------

admin/admin@123

4.181上的maxkb的用户名和密码

------

Qanything的链接：

https://ai.youdao.com/saas/qanything/#/zhixie/home

------

发布data-service后台：

./build.sh image export

docker push harbor.canone.com/canone/core/canone-data-service:XXXX
kubectl -n canone-core set image deploy data-service *=harbor.canone.com/canone/core/canone-data-service:XXXX



------

从远程目录加载，暂时文件都是放在容器的/opt/canone下面，测试的时候需要做如下事情：

1.  到容器里面看一下这个目录有没有文件，如果有文件，删除
2. 在宿主机执行kubectl cp xxxxxx -n canone-core data-service-xxxxxxxxxxxxx:/opt/canone/，两个实例都要执行该操作
3. 最后看一下gfs的挂载点是否有文件，以及文件是否已经落地data_set_file文件且是未解析状态。

已完成任务：

1. 原上传文件落地data_set_file表，未测试
2. 从挂载目录读取文件并上传gfs，同时落地data_set_file已完成。落地data_set_file未测试。
3. 单个文件的标注文件下载，
4. 标注文件生成在model-servie中，如何在data-service更新数据？

------

sshfs挂载：

sshfs -o allow_other,IdentityFile=~/.ssh/id_rsa $username@$ip:$remotedir $localdir

```bash
# 已经建立互信，用这个。
sshfs -o allow_other,IdentityFile=~/.ssh/id_rsa $username@$ip:$remotedir $localdir
# 没有建立互信，会要输入密码：
sshfs -o allow_other $username@$ip:$remotedir $localdir
```

解挂载：

```bash
fusermount -u $localdir
```

------

sshfs有一个问题，现在没人维护了。可以使用其他替代品，比如rclone：

https://github.com/rclone/rclone

------

kubectl cp $subpath/ canone-core/$pod_name:$localdir

kubectl cp * 。。。这个命令是有问题的，不支持*。如果你想把目录下的整个文件都copy到容器里面，还是退到上级目录，cp目录吧。

比如你现在在/opt/data/abcd. 你想通过kubectl cp * canone-core/$pod_name:/opt/data/abcd

你的需求很明显：将/opt/data/abcd下的所有文件copy到容器的/opt/data/abcd目录下，但是不幸的告诉你，会报错：

**error: one of src or dest must be a remote file specification**

可以通过如下这种方式达到这个目录。你退回到上级目录/opt/data,

在这个目录（/opt/data）下执行：kubectl cp abcd/ canone-core/$pod_name:/opt/data

(注意容器也要退到上级目录)，也就是把这个目录copy到/opt/data下。该目录下的所有文件也就copy到容器里面了。

------

目前存在一个问题：

systemd的service里面无法访问k8s的。

------

echo 3 > /proc/sys/vm/drop_caches

释放系统内存



------

发布model-service

./build.sh image export 
docker push harbor.canone.com/canone/core/canone-model-service:20241209155156
kubectl -n canone-core set image deploy model-service *=harbor.canone.com/canone/core/canone-model-service:20241209155156



------

查看当前目录的文件或目录占用：

```bash
# 查看当前目录的文件或目录占用
du -h --max-depth=1
# 查看这个目录占用的控件总量
du -sh

```

------

如何只下载不安装rpm包：

```bash

# 将fuse-sshfs的安装rpm包下载到downloaddir指定的目录，但是不安装
yum install --downloadonly --downloaddir=/home/fxy/sshfs fuse-sshfs
# 上述这种其实如果fuse-sshfs已经安装了，那么还是不会下载，会报已经安装了。
# 如果想让系统虽然安装了但是还是下载。则用下面的
yum reinstall --downloadonly fuse --downloaddir=/home/fxy/sshfs
# 查看某个软件
rpm -qa|grep fuse-sshfs
```

------

下面是一个脚本，可以不停的下载某个大模型（需要提前python安装huggingface-cli包）：

```shell
#!/bin/bash

for i in {1..1000}; do 
  export HF_ENDPOINT=https://hf-mirror.com;
  huggingface-cli download --resume-download Qwen/Qwen2.5-3B-Instruct --include "*" --local-dir /home/fxy/models/1031; 
  sleep 10; 
done
```



------

将其他格式文件转换为pdf的工具：

https://www.pdf.to/results/0eca255f6b26465fbdf8b4b700f8d583/



------

training-service部署
./build.sh image
docker push harbor.canone.com/canone/core/canone-training-service:20240826140636
kubectl -n canone-core set image deploy training-service *=harbor.canone.com/canone/core/canone-training-service:20240826140636



------

fuser -v /dev/nvidia*

查看正在使用nvidia gpu的进程。用nvidia-smi可能看不到。

------

[机器学习](https://cloud.baidu.com/product/ai_bml.html)中的优化器是用来寻找模型参数最优解的工具。在训练过程中，优化器通过不断调整模型参数，最小化损失函数，从而找到最优解。常见的优化器有SGD、SGDM、Adagrad、RMSProp和Adam等。
SGD（随机梯度下降）是最基础的优化器，每次只使用一个样本进行参数更新，计算量较小，但容易陷入局部最优解。SGDM（SGD with momentum）是SGD的改进版，通过加入动量项，可以加速收敛并减少震荡。

Adam是一种结合了动量项和自适应学习率的优化器，通过计算梯度的一阶矩和二阶矩来动态调整学习率。Adam具有较快的收敛速度和良好的稳定性，因此在许多领域得到广泛应用。
选择合适的优化器需要考虑任务的性质、数据规模、模型复杂度等因素。在选择优化器时，需要考虑其收敛速度、稳定性、易用性和可扩展性等方面。对于不同的任务和模型，可能需要尝试不同的优化器来找到最适合的那一个。

在实际应用中，我们通常会使用开源库提供的默认参数进行模型训练。然而，有时候我们可能需要手动调整优化器的参数以获得更好的性能。例如，可以尝试调整学习率、动量项的系数、指数衰减平均值的衰减率等参数来提高模型的训练效果。

 很多人在使用pytorch的时候都会遇到优化器选择的问题，今天就给大家介绍对比一下pytorch中常用的四种优化器。SGD、Momentum、RMSProp、Adam。







怎么样？
