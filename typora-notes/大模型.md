# 知识结构



# 内容

## 大模型微调

什么是模型微调，是因为某些时候模型在某一方面能力不够，所以通过一些训练的方法把模型做一个更新，本质上是对模型的一个改动。模型背后是一堆参数来构成的，那训练的过程就是从原有的参数变成一个新的参数。这里的参数可以理解为很多数字的集合。

一般分为：全量微调（FFT，Full Fine Tunning）。每个参数都要调。
一种为：量化微调或高效参数微调PEFT(Parameter-Efficient Fine Tuning)

FFT的问题：

- 一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；
- 一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。

PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。从训练数据的来源、以及训练的方法的角度，大模型的微调有以下几条技术路线：

- 一个是监督式微调SFT(Supervised Fine Tuning) ，这个方案主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调
- 一个是基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) ，这个方案的主要特点是把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望。
- 还有一个是基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) ，这个原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。

目前有比较流行的几种方案：

### Prompt Tuning/Prefix Tuning/LoRA/QLoRA

### PEFT-LoRA

1. 全量微调学到的参数承载的信息是有限的，有很多冗余的信息，传达的信息是非常有限的，花这么多资源训练全量参数实际上是浪费资源的。
2. 微调的本质实际上就是希望参数承载的信息比较有限，希望把模型的部分能力放大，但是同时，又想保留我们大模型的其他的能力。参数的改动量（原参数 + 改动量 = 新参数）其实就是针对我们想去提升的那部分的能力。也就是，我们不希望把原来的模型改动太多。如果改动太多就意味着什么了，也就是原来模型具备的比较强的能力就消失了。这个就是模型能力的遗忘。所以实际上我们想要的结果是什么，看似改动了很多的变量，但是实际上改动所带来的价值是比较有限的。

所以有没有这样一种方式，能更加高效的去学出这些参数。

定义矩阵W，W是包含很多参数的矩阵，W=A * B，比如W=100 * 100，那么可以拆成A=100k， B=k100。 比如k=2，那么W就是10000个参数，而A为100*2=200个参数（100行2列的矩阵），而B也是200个参数（2\*100)。A和B一共400个参数，而A是10000个参数。通过400个参数的学习，就可以近似达到1万个参数的目的。如果k=1，那么学习的参数就是原来的2%，大大降低了需要学习的参数，这个就是LoRA的核心思想。

这里的k就是Rank，如果我们认为这个矩阵所涵盖的信息量很少，就可以选择比较小的k，如果大，那就选择更大一点的k。 LoRA里面，k一般不会很大。

### RLHF(Reinforcement Learning with Human Feedback)

## 生成式AI不等于大模型

模型



