# 知识结构

hf的用户信息：  79632345@qq.com/Luntan*

username：jumpLLM

Full name：fengjumpLLM

token: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

![image-20240811211947967](大模型.assets/image-20240811211947967.png)





# 内容

## 大模型微调

什么是模型微调，是因为某些时候模型在某一方面能力不够，所以通过一些训练的方法把模型做一个更新，本质上是对模型的一个改动。模型背后是一堆参数来构成的，那训练的过程就是从原有的参数变成一个新的参数。这里的参数可以理解为很多数字的集合。

一般分为：全量微调（FFT，Full Fine Tunning）。每个参数都要调。
一种为：量化微调或高效参数微调PEFT(Parameter-Efficient Fine Tuning)

FFT的问题：

- 一个是训练的成本会比较高，因为微调的参数量跟预训练的是一样的多的；
- 一个是叫灾难性遗忘(Catastrophic Forgetting)，用特定训练数据去微调可能会把这个领域的表现变好，但也可能会把原来表现好的别的领域的能力变差。

PEFT主要想解决的问题，就是FFT存在的上述两个问题，PEFT也是目前比较主流的微调方案。从训练数据的来源、以及训练的方法的角度，大模型的微调有以下几条技术路线：

- 一个是监督式微调SFT(Supervised Fine Tuning) ，这个方案主要是用人工标注的数据，用传统机器学习中监督学习的方法，对大模型进行微调
- 一个是基于人类反馈的强化学习微调RLHF(Reinforcement Learning with Human Feedback) ，这个方案的主要特点是把人类的反馈，通过强化学习的方式，引入到对大模型的微调中去，让大模型生成的结果，更加符合人类的一些期望。
- 还有一个是基于AI反馈的强化学习微调RLAIF(Reinforcement Learning with AI Feedback) ，这个原理大致跟RLHF类似，但是反馈的来源是AI。这里是想解决反馈系统的效率问题，因为收集人类反馈，相对来说成本会比较高、效率比较低。

目前有比较流行的几种方案：

### Prompt Tuning/Prefix Tuning/LoRA/QLoRA

### PEFT-LoRA

1. 全量微调学到的参数承载的信息是有限的，有很多冗余的信息，传达的信息是非常有限的，花这么多资源训练全量参数实际上是浪费资源的。
2. 微调的本质实际上就是希望参数承载的信息比较有限，希望把模型的部分能力放大，但是同时，又想保留我们大模型的其他的能力。参数的改动量（原参数 + 改动量 = 新参数）其实就是针对我们想去提升的那部分的能力。也就是，我们不希望把原来的模型改动太多。如果改动太多就意味着什么了，也就是原来模型具备的比较强的能力就消失了。这个就是模型能力的遗忘。所以实际上我们想要的结果是什么，看似改动了很多的变量，但是实际上改动所带来的价值是比较有限的。

所以有没有这样一种方式，能更加高效的去学出这些参数。

定义矩阵W，W是包含很多参数的矩阵，W=A * B，比如W=100 * 100，那么可以拆成A=100k， B=k100。 比如k=2，那么W就是10000个参数，而A为100*2=200个参数（100行2列的矩阵），而B也是200个参数（2\*100)。A和B一共400个参数，而A是10000个参数。通过400个参数的学习，就可以近似达到1万个参数的目的。如果k=1，那么学习的参数就是原来的2%，大大降低了需要学习的参数，这个就是LoRA的核心思想。

这里的k就是Rank，如果我们认为这个矩阵所涵盖的信息量很少，就可以选择比较小的k，如果大，那就选择更大一点的k。 LoRA里面，k一般不会很大。

### RLHF(Reinforcement Learning with Human Feedback)

## 生成式AI不等于大模型

模型



## 大模型下载工具

1. 可以直接在hugging face官网手动下载各种大模型，但是这些模型都比较大。而且如果中断，可能需要重新下载。

2. 使用hfd工具，用法可参考：https://gist.github.com/padeoe/697678ab8e528b85a2a7bddafea1fa4f

   > 1. 下载：
   >
   >    wget https://hf-mirror.com/hfd/hfd.sh
   >    chmod a+x hfd.sh
   >
   > 2. 

3. 可以直接使用hf的镜像网站下载

   > 1. 镜像网站：https://hf-mirror.com/
   > 2. 在linux中将export HF_ENDPOINT=https://hf-mirror.com放到profile中，hfd将在镜像网站下载，速度可能会快一点（未实验过）
   > 3. hfd需要下载aria2c 工具等。

4. 实例。

   > ```bash
   > # 用aria2c工具下载gpt2模型。下载到当前目录
   > # 如果是用aria2c，那么-x表示用多少线程来下载。默认是4
   > ./hfd.sh gpt2 --tool aria2c -x 4
   > # 查看帮助文档
   > ./hfd.sh -h
   > # 下载bigscience/bloom-560m大模型
   > ./hfd.sh bigscience/bloom-560m
   > # 下载meta-llama/Llama-2-7b模型，使用hf的用户名和token
   > # 注意有的时候模型下载需要hf的token，可以从如下地址获取token。
   > # https://huggingface.co/settings/tokens
   > ./hfd.sh meta-llama/Llama-2-7b --hf_username YOUR_HF_USERNAME_NOT_EMAIL --hf_token YOUR_HF_TOKEN
   > 
   > # 下载模型排除掉所有的safetensors后缀文件
   > ./hfd.sh bigscience/bloom-560m --exclude *.safetensors
   > 
   > # 使用wget下载工具下载模型（默认是用aria2c）
   >  ./hfd.sh bigscience/bloom-560m --tool wget --exclude *.safetensors
   > ```

5. 使用huggingface_hub包

   > 1. pip install huggingface_hub
   >
   > 2. 如下一段代码就是下载代码
   >
   >    
   >
   >    ```python
   >    import os
   >    # 注意os.environ得在import huggingface库相关语句之前执行。
   >    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
   >    from huggingface_hub import hf_hub_download
   >    
   >    def download_model(source_url,local_dir):
   >    
   >        # 使用huggingface原地址
   >        # source_url ="https://huggingface.co/BlinkDL/rwkv-4-novel/blob/main/RWKV-4-Novel-7B-v1-ChnEng-20230426-ctx8192.pth"
   >        # 使用huggingface-镜像地址
   >        # source_url = "https://hf-mirror.com/BlinkDL/rwkv-4-novel/blob/main/RWKV-4-Novel-7B-v1-ChnEng-20230426-ctx8192.pth"
   >    
   >        if 'blob' in source_url:
   >            sp = '/blob/main/'
   >        else:
   >            sp = '/resolve/main/'
   >    
   >        if 'huggingface.co' in source_url:
   >            url = 'https://huggingface.co/'
   >        else:
   >            url = 'https://hf-mirror.com'
   >    
   >        location = source_url.split(sp)
   >        repo_id = location[0].strip(url) # 仓库ID，例如："BlinkDL/rwkv-4-world" 
   >    
   >            cache_dir = local_dir + "/cache"
   >        filename = location[1]# 大模型文件，例如："RWKV-4-World-CHNtuned-7B-v1-20230709-ctx4096.pth"
   >    
   >        print(f'开始下载\n仓库：{repo_id}\n大模型：{filename}\n如超时不用管，会自定继续下载，直至完成。中途中断，再次运行将继续下载。')
   >        while True:   
   >            try:
   >                hf_hub_download(cache_dir=cache_dir,
   >                local_dir=local_dir,
   >                repo_id=repo_id,
   >                filename=filename,
   >                local_dir_use_symlinks=False,
   >                resume_download=True,
   >                etag_timeout=100
   >                )
   >            except Exception as e :
   >                print(e)
   >            else:
   >                print(f'下载完成，大模型保存在：{local_dir}\{filename}')
   >                break
   >                
   >    if __name__ == '__main__':
   >        source_url = "https://huggingface.co/BlinkDL/rwkv-4-novel/blob/main/RWKV-4-Novel-7B-v1-ChnEng-20230426-ctx8192.pth"
   >        source_url = "https://hf-mirror.com/BlinkDL/rwkv-4-novel/blob/main/RWKV-4-Novel-7B-v1-ChnEng-20230426-ctx8192.pth"
   >        # source_url = "https://huggingface.co/BlinkDL/rwkv-5-world/resolve/main/RWKV-5-World-1B5-v2-20231025-ctx4096.pth"
   >        download_model(source_url,local_dir = r'D:\ProgramData\RWKV\models')
   >       
   >    ```
   >
   >    
   >
   > 3. 如果要下载整个仓库，可使用snapshot_download下载整个仓库快照

6. 如何下载大模型，https://www.jianshu.com/p/86c4a45f0a18



## 大模型微调步骤

1. ### Step 1: Choose a pre-trained model and a dataset

2. ### Step 2: Load the data to use

   ```python
   from datasets import load_dataset
   
   dataset = load_dataset("mteb/tweet_sentiment_extraction")
   df = pd.DataFrame(dataset['train'])
   ```

   

3. ### Step 3: Tokenizer

   ```python
   from transformers import GPT2Tokenizer
   
   # Loading the dataset to train our model
   dataset = load_dataset("mteb/tweet_sentiment_extraction")
   
   tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
   tokenizer.pad_token = tokenizer.eos_token
   def tokenize_function(examples):
      return tokenizer(examples["text"], padding="max_length", truncation=True)
   
   tokenized_datasets = dataset.map(tokenize_function, batched=True)
   ```

   

4. ### Step 4: Initialize our base model

   ```python
   from transformers import GPT2ForSequenceClassification
   
   model = GPT2ForSequenceClassification.from_pretrained("gpt2", num_labels=3)
   ```

   

5. ### Step 5: Evaluate method

   ```python
   import evaluate
   
   metric = evaluate.load("accuracy")
   
   def compute_metrics(eval_pred):
      logits, labels = eval_pred
      predictions = np.argmax(logits, axis=-1)
      return metric.compute(predictions=predictions, references=labels)
   ```

   

6. ### Step 6: Fine-tune using the Trainer Method

   ```python
   from transformers import TrainingArguments, Trainer
   
   training_args = TrainingArguments(
      output_dir="test_trainer",
      #evaluation_strategy="epoch",
      per_device_train_batch_size=1,  # Reduce batch size here
      per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well
      gradient_accumulation_steps=4
      )
   
   
   trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=small_train_dataset,
      eval_dataset=small_eval_dataset,
      compute_metrics=compute_metrics,
   
   )
   
   trainer.train()
   
   ```

   
